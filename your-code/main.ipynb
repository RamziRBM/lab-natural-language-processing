{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RamziRBM/lab-natural-language-processing/blob/main/your-code/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "F0cvc0c1LitW",
        "outputId": "4efb7414-a097-4320-8787-cbee96e1390b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>.container { width:100% !important; }</style>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.core.display import display, HTML\n",
        "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oxc2RlfPLitW"
      },
      "source": [
        "# Lab | Natural Language Processing\n",
        "### SMS: SPAM or HAM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcg4Gxe0LitX"
      },
      "source": [
        "### Let's prepare the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0dovv_M1LitX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re, string, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
        "plt.rcParams[\"figure.figsize\"] = (8,5)"
      ],
      "metadata": {
        "id": "zs-unf5MUe7M"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IrfWqShLitY"
      },
      "source": [
        "- Read Data for the Fraudulent Email Kaggle Challenge\n",
        "- Reduce the training set to speead up development."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8FBdx1X7LitY",
        "outputId": "fe374f79-04af-4a48-bb68-7b3935c65445",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5964, 2)\n"
          ]
        }
      ],
      "source": [
        "## Read Data for the Fraudulent Email Kaggle Challenge\n",
        "data_train = pd.read_csv(\"https://github.com/RamziRBM/lab-natural-language-processing/raw/refs/heads/main/data/kg_train.csv\",encoding='latin-1')\n",
        "\n",
        "# Reduce the training set to speed up development.\n",
        "# Modify for final system\n",
        "#data_train = data_train.head(1000)\n",
        "print(data_train.shape)\n",
        "data_train.fillna(\"\",inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_raw = data_train[\"text\"]\n",
        "y = data_train[\"label\"]\n",
        "\n",
        "X_train_raw, X_valid_raw, y_train, y_valid = train_test_split(\n",
        "    X_raw, y, test_size=0.20, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "print(\"Train/valid sizes:\", X_train_raw.shape, X_valid_raw.shape)"
      ],
      "metadata": {
        "id": "FrlwYnm9TEs4",
        "outputId": "3e702870-868e-4cc6-f4fa-c6837be857bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train/valid sizes: (4771,) (1193,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGSCeiKuLitY"
      },
      "source": [
        "### Let's divide the training and test set into two partitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QCFnFlRqLitZ",
        "outputId": "b4d0353c-3817-486b-8be4-7ec59b6e8622",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5964, 1)\n"
          ]
        }
      ],
      "source": [
        "# Your code\n",
        "## Read Data for the Fraudulent Email Kaggle Challenge\n",
        "data_test = pd.read_csv(\"https://github.com/RamziRBM/lab-natural-language-processing/raw/refs/heads/main/data/kg_test.csv\",encoding='latin-1')\n",
        "\n",
        "# Reduce the training set to speed up development.\n",
        "# Modify for final system\n",
        "#data_test = data_test.head(1000)\n",
        "print(data_test.shape)\n",
        "data_test.fillna(\"\",inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytmhsuJQLitZ"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "for pkg, path in [(\"punkt\", \"tokenizers/punkt\"),\n",
        "                  (\"stopwords\",\"corpora/stopwords\")]:\n",
        "    try:\n",
        "        nltk.data.find(path)\n",
        "    except LookupError:\n",
        "        nltk.download(pkg)\n",
        "\n",
        "STOPWORDS = set(stopwords.words(\"english\"))"
      ],
      "metadata": {
        "id": "N_emHFVCTbkV",
        "outputId": "6c542d38-a649-485e-93e0-d2617aaf6f33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "oGD1L0A2LitZ",
        "outputId": "a0ee8de1-1a41-420d-a188-ea6680515206",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
            "['needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on']\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "print(string.punctuation)\n",
        "print(stopwords.words(\"english\")[100:110])\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "snowball = SnowballStemmer('english')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEGWsa_ALita"
      },
      "source": [
        "## Now, we have to clean the html code removing words\n",
        "\n",
        "- First we remove inline JavaScript/CSS\n",
        "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
        "- Next we can remove the remaining tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "NdKZNkFLLita"
      },
      "outputs": [],
      "source": [
        "# Your code\n",
        "from bs4 import BeautifulSoup, Comment\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# HTML strip\n",
        "\n",
        "def strip_html_layers(html):\n",
        "    \"\"\"\n",
        "    1) Remove inline JS/CSS\n",
        "    2) Remove HTML comments\n",
        "    3) Remove remaining tags\n",
        "    Return plain text (no tags).\n",
        "    \"\"\"\n",
        "    if pd.isna(html):\n",
        "        return \"\"\n",
        "\n",
        "    soup = BeautifulSoup(str(html), \"html.parser\")\n",
        "\n",
        "    # (1) Remove inline JS/CSS\n",
        "    #    a) <script> and <style> blocks\n",
        "    for tag in soup([\"script\", \"style\"]):\n",
        "        tag.decompose()\n",
        "    #    b) inline event handlers / inline style attributes (onclick, onload, style, etc.)\n",
        "    for tag in soup(True):\n",
        "        # collect then delete to avoid runtime mutation issues\n",
        "        for attr in list(tag.attrs):\n",
        "            al = attr.lower()\n",
        "            if al == \"style\" or al.startswith(\"on\"):  # on* = onclick, onload...\n",
        "                del tag.attrs[attr]\n",
        "\n",
        "    # (2) Remove HTML comments (do this before stripping tags in regex approaches; with BS it's explicit)\n",
        "    for c in soup.find_all(string=lambda t: isinstance(t, Comment)):\n",
        "        c.extract()\n",
        "\n",
        "    # (3) Remove remaining tags: get only the text content\n",
        "    text = soup.get_text(separator=\" \", strip=True)\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Optional: normalize the extracted text (letters only, lowercase, collapse spaces)\n",
        "\n",
        "for pkg, path in [(\"punkt\",\"tokenizers/punkt\"), (\"stopwords\",\"corpora/stopwords\")]:\n",
        "    try:\n",
        "        nltk.data.find(path)\n",
        "    except LookupError:\n",
        "        nltk.download(pkg)\n",
        "\n",
        "STOPWORDS = set(stopwords.words(\"english\"))\n",
        "\n",
        "def normalize_text(plain_text: str, remove_stopwords=True):\n",
        "    if pd.isna(plain_text):\n",
        "        return \"\"\n",
        "    t = plain_text.lower()\n",
        "    t = re.sub(r\"\\d+\", \" \", t)          # remove digits\n",
        "    t = re.sub(r\"[^a-z\\s]\", \" \", t)     # keep letters/spaces\n",
        "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
        "\n",
        "    if not remove_stopwords:\n",
        "        return t\n",
        "\n",
        "    tokens = [w for w in word_tokenize(t) if w not in STOPWORDS and len(w) > 1]\n",
        "    return \" \".join(tokens)\n",
        "\n"
      ],
      "metadata": {
        "id": "--Hjo9mYTwVI"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjjQRCFwLita"
      },
      "source": [
        "- Remove all the special characters\n",
        "    \n",
        "- Remove numbers\n",
        "    \n",
        "- Remove all single characters\n",
        "\n",
        "- Remove single characters from the start\n",
        "\n",
        "- Substitute multiple spaces with single space\n",
        "\n",
        "- Remove prefixed 'b'\n",
        "\n",
        "- Convert to Lowercase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "GxFqAcpCLita"
      },
      "outputs": [],
      "source": [
        "# Your code\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# 3) Normalize exact\n",
        "def normalize_exact(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Apply these steps IN ORDER:\n",
        "    1) Remove all special characters (keep letters/numbers/spaces)\n",
        "    2) Remove numbers\n",
        "    3) Remove all single characters\n",
        "    4) Remove single characters from the start\n",
        "    5) Substitute multiple spaces with single space\n",
        "    6) Remove prefixed b (e.g., b'...') if present\n",
        "    7) Convert to lowercase\n",
        "    \"\"\"\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    s = str(text)\n",
        "\n",
        "    # 1) Remove special characters (keep letters, digits, whitespace)\n",
        "    s = re.sub(r\"[^A-Za-z0-9\\s]\", \" \", s)\n",
        "\n",
        "    # 2) Remove numbers\n",
        "    s = re.sub(r\"\\d+\", \" \", s)\n",
        "\n",
        "    # 3) Remove all single characters (isolated a–z/A–Z)\n",
        "    s = re.sub(r\"\\b[a-zA-Z]\\b\", \" \", s)\n",
        "\n",
        "    # 4) Remove single characters from the start (e.g., leading 'a ' at line start)\n",
        "    s = re.sub(r\"^\\s*[a-zA-Z]\\s+\", \" \", s)\n",
        "\n",
        "    # 5) Substitute multiple spaces with single space\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "\n",
        "    # 6) Remove prefixed 'b' (handles byte-literal-like strings such as b'...'/b\"...\")\n",
        "    #    If your text literally includes leading b'...' from decoding issues, this strips it.\n",
        "    m = re.match(r\"\"\"^b(?:['\"])(.*)(?:['\"])$\"\"\", s)\n",
        "    if m:\n",
        "        s = m.group(1).strip()\n",
        "\n",
        "    # 7) Convert to lowercase\n",
        "    s = s.lower()\n",
        "\n",
        "    return s\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import MarkupResemblesLocatorWarning\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
        "\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "AH8OjRkSUmIb",
        "outputId": "4e9b07ec-d267-421c-ab6f-17b3bb8edebb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) HTML → text (structure removal only)\n",
        "X_train_nohtml = X_train_raw.apply(strip_html_layers)\n",
        "X_valid_nohtml = X_valid_raw.apply(strip_html_layers)\n",
        "\n",
        "# 2) Plain-text normalization/token filtering (still per split)\n",
        "X_train_clean = X_train_nohtml.apply(normalize_text)\n",
        "X_valid_clean = X_valid_nohtml.apply(normalize_text)\n",
        "\n",
        "# 3) Normalize exact\n",
        "X_train_clean = X_train_raw.apply(normalize_exact)\n",
        "X_valid_clean = X_valid_raw.apply(normalize_exact)"
      ],
      "metadata": {
        "id": "2SswNni1T3oU"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgV_mUCKLita"
      },
      "source": [
        "## Now let's work on removing stopwords\n",
        "Remove the stopwords."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6QGRJsSLita"
      },
      "outputs": [],
      "source": [
        "# Your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkehDIbxLitb"
      },
      "source": [
        "## Tame Your Text with Lemmatization\n",
        "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "9VbCV9a6Litb"
      },
      "outputs": [],
      "source": [
        "# Your code\n",
        "import re\n",
        "from typing import List\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# --- Optional lemmatizers (try-import; safe if missing) ---\n",
        "_use_spacy = False\n",
        "_use_wordnet = False\n",
        "try:\n",
        "    import spacy\n",
        "    _nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\",\"ner\",\"textcat\"])\n",
        "    _use_spacy = True\n",
        "except Exception:\n",
        "    try:\n",
        "        # Try NLTK WordNet if present\n",
        "        import nltk\n",
        "        from nltk.stem import WordNetLemmatizer\n",
        "        from nltk.corpus import wordnet as wn\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        _use_wordnet = True\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# --- Tiny rule-based lemmatizer (fallback) ---\n",
        "_IRREGULAR = {\n",
        "    \"bought\":\"buy\",\"brought\":\"bring\",\"went\":\"go\",\"gone\":\"go\",\"saw\":\"see\",\"seen\":\"see\",\n",
        "    \"was\":\"be\",\"were\":\"be\",\"am\":\"be\",\"is\":\"be\",\"are\":\"be\",\"been\":\"be\",\n",
        "    \"did\":\"do\",\"done\":\"do\",\"has\":\"have\",\"had\":\"have\",\n",
        "    \"ran\":\"run\",\"better\":\"good\",\"best\":\"good\",\"worse\":\"bad\",\"worst\":\"bad\",\n",
        "}\n",
        "def _rule_based_lemma(tok: str) -> str:\n",
        "    if tok in _IRREGULAR:\n",
        "        return _IRREGULAR[tok]\n",
        "    # Fix common Porter artifacts: happi -> happy, studi -> study, respons -> respons (leave)\n",
        "    if tok.endswith(\"i\") and len(tok) > 2:\n",
        "        return tok[:-1] + \"y\"\n",
        "    # Plural-ish cleanups\n",
        "    if tok.endswith(\"ves\") and len(tok) > 3:      # knives -> knive (approx) -> prefer 'f' rule\n",
        "        return tok[:-3] + \"f\"\n",
        "    if tok.endswith(\"ies\") and len(tok) > 3:      # parties -> party\n",
        "        return tok[:-3] + \"y\"\n",
        "    if tok.endswith(\"s\") and len(tok) > 3 and not tok.endswith(\"ss\"):\n",
        "        return tok[:-1]\n",
        "    return tok\n",
        "\n",
        "def _tokenize_letters(s: str) -> List[str]:\n",
        "    # keep letters only, lowercase\n",
        "    return re.findall(r\"[a-zA-Z]+\", s.lower())\n",
        "\n",
        "def stem_then_lemmatize(text: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        text = \"\" if text is None else str(text)\n",
        "\n",
        "    # 1) tokenize (letters only)\n",
        "    toks = _tokenize_letters(text)\n",
        "    if not toks:\n",
        "        return \"\"\n",
        "\n",
        "    # 2) stemming first\n",
        "    stems = [stemmer.stem(t) for t in toks]\n",
        "\n",
        "    # 3) then lemmatization\n",
        "    if _use_spacy:\n",
        "        # Let spaCy lemmatize the *stemmed* string\n",
        "        doc = _nlp(\" \".join(stems))\n",
        "        lemmas = [t.lemma_.lower() for t in doc if t.lemma_.strip()]\n",
        "    elif _use_wordnet:\n",
        "        # Try both noun/verb, pick the shorter (very simple heuristic)\n",
        "        def wn_lemma(tok):\n",
        "            n = lemmatizer.lemmatize(tok, pos=\"n\")\n",
        "            v = lemmatizer.lemmatize(tok, pos=\"v\")\n",
        "            return n if len(n) <= len(v) else v\n",
        "        lemmas = [wn_lemma(t) for t in stems]\n",
        "    else:\n",
        "        # Rule-based fallback (no downloads needed)\n",
        "        lemmas = [_rule_based_lemma(t) for t in stems]\n",
        "\n",
        "    # 4) collapse whitespace\n",
        "    return \" \".join(lemmas)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_norm = X_train_raw.apply(stem_then_lemmatize)\n",
        "X_valid_norm = X_valid_raw.apply(stem_then_lemmatize)"
      ],
      "metadata": {
        "id": "X28xAjOaU7yj"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_QuJZiNLitb"
      },
      "source": [
        "## Bag Of Words\n",
        "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "gxweMeRxLitb",
        "outputId": "93d78e31-e645-42b9-b989-9cdaa7098540",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 HAM words:\n",
            "the          14282\n",
            "to           8439\n",
            "and          6836\n",
            "of           6542\n",
            "in           5114\n",
            "a            5109\n",
            "s            3574\n",
            "that         3247\n",
            "is           2914\n",
            "for          2796\n",
            "\n",
            "Top 10 SPAM words:\n",
            "the          39757\n",
            "to           31509\n",
            "of           27934\n",
            "and          21762\n",
            "i            20573\n",
            "in           17976\n",
            "you          17941\n",
            "a            16032\n",
            "this         14583\n",
            "e            14526\n",
            "\n",
            "=== Comparative Word Analysis (Top 10 words) ===\n",
            "HAM Word        Count      | SPAM Word       Count      | Dominant  \n",
            "----------------------------------------------------------------------\n",
            "the             14282      | the             39757      | SPAM (2x) \n",
            "to              8439       | to              31509      | SPAM (3x) \n",
            "and             6836       | of              27934      | SPAM (4x) \n",
            "of              6542       | and             21762      | SPAM (3x) \n",
            "in              5114       | i               20573      | SPAM (4x) \n",
            "a               5109       | in              17976      | SPAM (3x) \n",
            "s               3574       | you             17941      | SPAM (5x) \n",
            "that            3247       | a               16032      | SPAM (4x) \n",
            "is              2914       | this            14583      | SPAM (5x) \n",
            "for             2796       | e               14526      | SPAM (5x) \n"
          ]
        }
      ],
      "source": [
        "# Your code\n",
        "# 1. Build token solumns for ham & spam\n",
        "# assuming the training dataframe is data_train with columns:\n",
        "# text;\n",
        "# label (0 = ham, 1=spam)\n",
        "\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "def tokenize(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
        "    tokens = text.split()\n",
        "    return tokens\n",
        "\n",
        "data_train[\"tokens\"] = data_train[\"text\"].apply(tokenize)\n",
        "\n",
        "# 2. Split ham & spam tokens\n",
        "ham_tokens = [token for tokens in data_train[data_train[\"label\"] == 0][\"tokens\"] for token in tokens]\n",
        "spam_tokens = [token for tokens in data_train[data_train[\"label\"] == 1][\"tokens\"] for token in tokens]\n",
        "\n",
        "ham_counts = Counter(ham_tokens)\n",
        "spam_counts = Counter(spam_tokens)\n",
        "\n",
        "top_ham = ham_counts.most_common(10)\n",
        "top_spam = spam_counts.most_common(10)\n",
        "\n",
        "# 3. Show Top 10 Words in Each Class\n",
        "print(\"Top 10 HAM words:\")\n",
        "for word, count in ham_counts.most_common(10):\n",
        "    print(f\"{word:<12} {count}\")\n",
        "\n",
        "print(\"\\nTop 10 SPAM words:\")\n",
        "for word, count in spam_counts.most_common(10):\n",
        "    print(f\"{word:<12} {count}\")\n",
        "\n",
        "# 4. Comparative Analysis\n",
        "def comparative_analysis(top_ham, top_spam, n=10):\n",
        "    \"\"\"\n",
        "    Compare top Bag-of-Words terms between HAM and SPAM messages.\n",
        "    Input should be Counter().most_common() lists.\n",
        "    \"\"\"\n",
        "    print(\"\\n=== Comparative Word Analysis (Top {} words) ===\".format(n))\n",
        "    print(\"{:<15} {:<10} | {:<15} {:<10} | {:<10}\".format(\"HAM Word\", \"Count\", \"SPAM Word\", \"Count\", \"Dominant\"))\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    for i in range(n):\n",
        "        ham_word, ham_count = top_ham[i] if i < len(top_ham) else (\"-\", 0)\n",
        "        spam_word, spam_count = top_spam[i] if i < len(top_spam) else (\"-\", 0)\n",
        "\n",
        "        # Determine dominance\n",
        "        if ham_count > spam_count:\n",
        "            dominant = f\"HAM ({ham_count // (spam_count + 1)}x)\"\n",
        "        elif spam_count > ham_count:\n",
        "            dominant = f\"SPAM ({spam_count // (ham_count + 1)}x)\"\n",
        "        else:\n",
        "            dominant = \"Equal\"\n",
        "\n",
        "        print(\"{:<15} {:<10} | {:<15} {:<10} | {:<10}\".format(\n",
        "            ham_word, ham_count, spam_word, spam_count, dominant\n",
        "        ))\n",
        "\n",
        "comparative_analysis(top_ham, top_spam)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGvW0K0BLitb"
      },
      "source": [
        "## Extra features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJcKqmRLLitb"
      },
      "outputs": [],
      "source": [
        "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
        "money_simbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"€\",r\"\\$\"])\n",
        "suspicious_words = \"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"])\n",
        "\n",
        "data_train['money_mark'] = data_train['preprocessed_text'].str.contains(money_simbol_list)*1\n",
        "data_train['suspicious_words'] = data_train['preprocessed_text'].str.contains(suspicious_words)*1\n",
        "data_train['text_len'] = data_train['preprocessed_text'].apply(lambda x: len(x))\n",
        "\n",
        "data_val['money_mark'] = data_val['preprocessed_text'].str.contains(money_simbol_list)*1\n",
        "data_val['suspicious_words'] = data_val['preprocessed_text'].str.contains(suspicious_words)*1\n",
        "data_val['text_len'] = data_val['preprocessed_text'].apply(lambda x: len(x))\n",
        "\n",
        "data_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeHNgKKnLitb"
      },
      "source": [
        "## How would work the Bag of Words with Count Vectorizer concept?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "0jVRsEAVLitb",
        "outputId": "885151d5-8823-4bb0-a100-0cd21621de1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train matrix shape: (4771, 69056)\n",
            "Valid matrix shape: (1193, 69056)\n",
            "Sparsity: 0.9988 (fraction of zeros)\n",
            "Sample vocab: ['00' '000' '0000' '00000' '000000' '00000e2511c8' '00000e2511c8print'\n",
            " '0000642ec858344d00000000' '0000647cc858344d00000000' '000066' '000080'\n",
            " '0000ff' '00020500' '0008' '000999' '000m' '000million' '000ud' '000us'\n",
            " '000usd']\n"
          ]
        }
      ],
      "source": [
        "# Your code\n",
        "# Your code\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# assume X_train_raw, X_valid_raw exist (from train_test_split)\n",
        "vectorizer = CountVectorizer(\n",
        "    lowercase=True,\n",
        "    stop_words=\"english\",     # optional\n",
        "    ngram_range=(1,1)         # (1,1)=unigrams. Can use (1,2) for bigrams\n",
        ")\n",
        "\n",
        "# Learn vocabulary on train only\n",
        "X_train_bow = vectorizer.fit_transform(X_train_raw)\n",
        "\n",
        "# Transform validation with same vocabulary\n",
        "X_valid_bow = vectorizer.transform(X_valid_raw)\n",
        "\n",
        "print(\"Train matrix shape:\", X_train_bow.shape)\n",
        "print(\"Valid matrix shape:\", X_valid_bow.shape)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "nnz_train = X_train_bow.nnz\n",
        "total = np.prod(X_train_bow.shape)\n",
        "print(f\"Sparsity: {1 - nnz_train/total:.4f} (fraction of zeros)\")\n",
        "\n",
        "vocab = vectorizer.get_feature_names_out()\n",
        "print(\"Sample vocab:\", vocab[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35462n69Litc"
      },
      "source": [
        "## TF-IDF\n",
        "\n",
        "- Load the vectorizer\n",
        "\n",
        "- Vectorize all dataset\n",
        "\n",
        "- print the shape of the vetorized dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf = TfidfVectorizer(\n",
        "    lowercase=True,\n",
        "    ngram_range=(1,2),\n",
        "    min_df=3,\n",
        "    max_df=0.90,\n",
        "    sublinear_tf=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "yOh-NKcUWaLS"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split-safe (recommended for modeling)\n",
        "X_train_tfidf = tfidf.fit_transform(X_train_raw)   # learn vocab on TRAIN only\n",
        "X_valid_tfidf = tfidf.transform(X_valid_raw)       # reuse same vocab\n",
        "\n",
        "print(\"Train TF-IDF shape:\", X_train_tfidf.shape)\n",
        "print(\"Valid TF-IDF shape:\", X_valid_tfidf.shape)\n",
        "print(\"Vocab size:\", len(tfidf.get_feature_names_out()))"
      ],
      "metadata": {
        "id": "96IdhuTXWiEh",
        "outputId": "fdf66172-012c-4749-e25b-f7e65034ecfc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train TF-IDF shape: (4771, 57912)\n",
            "Valid TF-IDF shape: (1193, 57912)\n",
            "Vocab size: 57912\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizes the entire dataset (for EDA only)\n",
        "all_text = data_train[\"text\"].fillna(\"\").astype(str)\n",
        "X_all_tfidf = tfidf.fit_transform(all_text)  # fits on ALL data\n",
        "\n",
        "print(\"All-data TF-IDF shape:\", X_all_tfidf.shape)\n",
        "print(\"Vocab size (all-data):\", len(tfidf.get_feature_names_out()))"
      ],
      "metadata": {
        "id": "iaH7OzeFWmZJ",
        "outputId": "51063f67-186f-4d47-d527-2ae398b7ff61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All-data TF-IDF shape: (5964, 70483)\n",
            "Vocab size (all-data): 70483\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sv54qS1qLitc"
      },
      "source": [
        "## And the Train a Classifier?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "cX1soicoLitc",
        "outputId": "f8981691-24c4-4ca1-9ec1-e95c2f2d4174",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: (4771, 57912)  Valid: (1193, 57912)\n",
            "Accuracy: 0.9790444258172674\n",
            "ROC-AUC: 0.9970744163145661\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      1.00      0.98       677\n",
            "           1       1.00      0.95      0.98       516\n",
            "\n",
            "    accuracy                           0.98      1193\n",
            "   macro avg       0.98      0.98      0.98      1193\n",
            "weighted avg       0.98      0.98      0.98      1193\n",
            "\n",
            "Confusion matrix:\n",
            " [[677   0]\n",
            " [ 25 491]]\n"
          ]
        }
      ],
      "source": [
        "# Your code\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer(\n",
        "    lowercase=True,\n",
        "    ngram_range=(1,2),\n",
        "    min_df=3,\n",
        "    max_df=0.90,\n",
        "    sublinear_tf=True\n",
        ")\n",
        "\n",
        "X_train_tfidf = tfidf.fit_transform(X_train_raw)   # fit on TRAIN only\n",
        "X_valid_tfidf = tfidf.transform(X_valid_raw)\n",
        "print(\"Train:\", X_train_tfidf.shape, \" Valid:\", X_valid_tfidf.shape)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
        "\n",
        "clf = LogisticRegression(max_iter=1000, n_jobs=-1)  # add class_weight='balanced' if needed\n",
        "clf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_valid_tfidf)\n",
        "y_proba = clf.predict_proba(X_valid_tfidf)[:,1] if hasattr(clf, \"predict_proba\") else None\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_valid, y_pred))\n",
        "if y_proba is not None and set(y_train.unique()) <= {0,1}:\n",
        "    print(\"ROC-AUC:\", roc_auc_score(y_valid, y_proba))\n",
        "print(classification_report(y_valid, y_pred))\n",
        "print(\"Confusion matrix:\\n\", confusion_matrix(y_valid, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "svm = LinearSVC()  # you can tune C (default 1.0)\n",
        "svm.fit(X_train_tfidf, y_train)\n",
        "\n",
        "y_pred = svm.predict(X_valid_tfidf)\n",
        "print(\"Accuracy:\", accuracy_score(y_valid, y_pred))\n",
        "print(classification_report(y_valid, y_pred))\n",
        "print(\"Confusion matrix:\\n\", confusion_matrix(y_valid, y_pred))"
      ],
      "metadata": {
        "id": "Pr6J_5i_W5-g",
        "outputId": "f268b485-7ff7-4c91-a0c3-1c4de4c8bfb9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9899413243922883\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99       677\n",
            "           1       1.00      0.98      0.99       516\n",
            "\n",
            "    accuracy                           0.99      1193\n",
            "   macro avg       0.99      0.99      0.99      1193\n",
            "weighted avg       0.99      0.99      0.99      1193\n",
            "\n",
            "Confusion matrix:\n",
            " [[677   0]\n",
            " [ 12 504]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5n2EMiRLitc"
      },
      "source": [
        "### Extra Task - Implement a SPAM/HAM classifier\n",
        "\n",
        "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
        "\n",
        "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
        "\n",
        "Your task is to **find the most relevant features**.\n",
        "\n",
        "For example, you can test the following options and check which of them performs better:\n",
        "- Using \"Bag of Words\" only\n",
        "- Using \"TF-IDF\" only\n",
        "- Bag of Words + extra flags (money_mark, suspicious_words, text_len)\n",
        "- TF-IDF + extra flags\n",
        "\n",
        "\n",
        "You can work with teams of two persons (recommended)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79golK-jLitc"
      },
      "outputs": [],
      "source": [
        "# Your code"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}